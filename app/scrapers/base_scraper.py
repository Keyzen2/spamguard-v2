"""
Clase base para todos los scrapers de vulnerabilidades.

Esta clase define la estructura comÃºn que todos los scrapers deben seguir.
Como principiante en Python, piensa en esto como una "plantilla" que 
todos los scrapers especÃ­ficos van a usar.
"""
import httpx
import asyncio
from typing import List, Dict, Optional
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_exponential

class BaseScraper:
    """
    Clase base para scrapers de vulnerabilidades.
    
    Todos los scrapers (WordPress, NVD, GitHub) van a heredar de esta clase.
    Herdar significa que van a tener todos estos mÃ©todos automÃ¡ticamente.
    """
    
    def __init__(self, name: str):
        """
        Constructor - se ejecuta cuando creas un scraper.
        
        Args:
            name: Nombre del scraper (ejemplo: 'wordpress', 'nvd')
        """
        self.name = name
        self.vulnerabilities = []  # Lista para guardar vulnerabilidades encontradas
        
        # ConfiguraciÃ³n de httpx (para hacer requests HTTP)
        self.client_config = {
            'timeout': 30.0,  # 30 segundos de timeout
            'follow_redirects': True,  # Seguir redirecciones
            'headers': {
                'User-Agent': 'SpamGuard-Security-Scanner/1.0'  # Identificarnos
            }
        }
    
    @retry(
        stop=stop_after_attempt(3),  # Reintentar 3 veces si falla
        wait=wait_exponential(multiplier=1, min=2, max=10)  # Esperar 2, 4, 8 segundos
    )
    async def fetch_url(self, url: str, params: Optional[Dict] = None) -> httpx.Response:
        """
        Hacer una peticiÃ³n HTTP con reintentos automÃ¡ticos.
        
        El decorador @retry hace que si falla, lo intente 3 veces automÃ¡ticamente
        con pausas exponenciales (2s, 4s, 8s).
        
        Args:
            url: URL a consultar
            params: ParÃ¡metros de la URL (opcional)
            
        Returns:
            Respuesta HTTP
            
        Ejemplo de uso:
            response = await self.fetch_url('https://api.example.com', {'page': 1})
        """
        async with httpx.AsyncClient(**self.client_config) as client:
            print(f"  ğŸ“¡ Fetching: {url}")
            response = await client.get(url, params=params)
            response.raise_for_status()  # Lanzar error si status no es 200
            return response
    
    async def scrape(self) -> List[Dict]:
        """
        MÃ©todo principal de scraping.
        
        ESTE MÃ‰TODO DEBE SER IMPLEMENTADO por cada scraper especÃ­fico.
        Es como decir: "cada scraper tiene que tener su propia forma de scrapear".
        
        Returns:
            Lista de vulnerabilidades encontradas
        """
        raise NotImplementedError(
            f"El scraper '{self.name}' debe implementar el mÃ©todo scrape()"
        )
    
    def normalize_vulnerability(self, raw_data: Dict) -> Dict:
      """
      Normalizar datos de vulnerabilidad a nuestro formato estÃ¡ndar.
      """
      return {
          'cve_id': raw_data.get('cve_id'),
          'source_id': raw_data.get('source_id'),
          'component_type': raw_data.get('component_type', 'plugin'),
          'component_slug': raw_data.get('component_slug'),
          'component_name': raw_data.get('component_name'),
          'affected_versions': raw_data.get('affected_versions', []),
          'patched_in': raw_data.get('patched_in'),
          'severity': raw_data.get('severity', 'medium'),
          'cvss_score': raw_data.get('cvss_score'),
          'title': raw_data.get('title'),
          'description': raw_data.get('description'),
          'vuln_type': raw_data.get('vuln_type'),
          'reference_urls': raw_data.get('reference_urls', {}),  # â† CAMBIADO
          'published_date': raw_data.get('published_date'),
          'discovered_by': raw_data.get('discovered_by'),
          'source': self.name,
          'verified': False,
          'active': True
      }
    
    def calculate_severity(self, cvss_score: Optional[float]) -> str:
        """
        Calcular severidad basada en CVSS score.
        
        CVSS es un estÃ¡ndar para medir la severidad de vulnerabilidades (0-10).
        Este mÃ©todo convierte el nÃºmero a una palabra.
        
        Args:
            cvss_score: Score CVSS (0-10)
            
        Returns:
            Severidad en texto: 'critical', 'high', 'medium', 'low'
        """
        if not cvss_score:
            return 'medium'
        
        if cvss_score >= 9.0:
            return 'critical'
        elif cvss_score >= 7.0:
            return 'high'
        elif cvss_score >= 4.0:
            return 'medium'
        else:
            return 'low'
    
    async def run(self) -> List[Dict]:
        """
        Ejecutar el scraping completo.
        
        Este mÃ©todo:
        1. Llama a scrape() para obtener datos
        2. Normaliza cada vulnerabilidad
        3. Retorna la lista normalizada
        
        Returns:
            Lista de vulnerabilidades normalizadas
        """
        print(f"\nğŸš€ Starting {self.name} scraper...")
        
        try:
            # Llamar al mÃ©todo scrape() especÃ­fico de cada scraper
            raw_vulnerabilities = await self.scrape()
            
            print(f"  âœ… Found {len(raw_vulnerabilities)} vulnerabilities")
            
            # Normalizar todas las vulnerabilidades
            normalized = [
                self.normalize_vulnerability(vuln) 
                for vuln in raw_vulnerabilities
            ]
            
            print(f"  âœ… Normalized {len(normalized)} vulnerabilities")
            
            return normalized
            
        except Exception as e:
            print(f"  âŒ Error in {self.name} scraper: {e}")
            return []
